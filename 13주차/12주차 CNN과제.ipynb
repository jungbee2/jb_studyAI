{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "703a29df-fcf6-41e2-a7f6-8f9123a1d087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "289349f7-132d-4a22-ae7e-49d9f0c6b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.util import smooth_curve\n",
    "# coding: utf-8\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "416b1d0a-13ea-44e5-9bdc-a7dbcbae826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x, flg):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        #print(self.x.shape, self.W.shape)\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "900abb2e-bfb9-445b-a126-c27b6a04fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu 활성화 함수 순전파, 역전파를 저장하는 클래스\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    # def forward(self, x, flg):\n",
    "    #     self.mask = (x <= 0)\n",
    "    #     out = x.copy()\n",
    "    #     out[self.mask] = 0\n",
    "\n",
    "    #     return out\n",
    "    def forward(self, x, flg=False):  # 'flg' 인수 추가\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out, flg  # 수정된 부분\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0269eb61-553f-439e-a374-83fc8f58d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7b8e697-02c5-4dfe-b1ae-58b68d3a9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03e460e1-bfc6-4a06-818b-02c9051cf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "161c7d2d-c4cc-4ce3-89c3-5ceac9417267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30cb3c94-71ab-480f-ad5c-911166b74ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, Ir=0.01):\n",
    "        self.Ir =Ir\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.Ir * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f0c4a9c-4131-4946-962d-154d22eddf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df7ccc05-fa98-4a9a-8e0a-adf50ac4d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__ (self, Ir=0.01):\n",
    "        self.Ir =Ir\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys() :\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.Ir * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e592201f-c9dc-49a6-a6d9-3f52dad159bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=1, beta=0, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
    "\n",
    "        # 시험할 때 사용할 평균과 분산\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward 시에 사용할 중간 데이터\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e97a344-d757-4dfd-8c54-74d3316db214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def He_init(input, ouput):\n",
    "    return np.sqrt(1.5 / input) * np.random.randn(input, ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efbc06eb-3fe1-40fb-9bde-7398eda3a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.maxIndex = len(hidden_size)\n",
    "        print(\"\\n\\n신경망 학습 시작 - 신경망: %d층\"%(self.maxIndex+1))\n",
    "        \n",
    "        self.params = {}        \n",
    "        self.params['W1'] = He_init(input_size, hidden_size[0])\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "\n",
    "        #print(self.params['W1'].shape)\n",
    "\n",
    "        for i in range(1, self.maxIndex):\n",
    "            self.params['W' + str(i+1)] = He_init(hidden_size[i-1], hidden_size[i]) \n",
    "            self.params['b' + str(i+1)] = np.zeros(hidden_size[i])\n",
    "            #print(self.params['W'  + str(i+1)].shape)\n",
    "\n",
    "        self.params['W' + str(self.maxIndex)] = He_init(hidden_size[self.maxIndex - 1], output_size) \n",
    "        self.params['b' + str(self.maxIndex)] = np.zeros(output_size)\n",
    "        #print(self.params['W' + str(self.maxIndex)].shape)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        for i in range(1, self.maxIndex):\n",
    "            self.layers['Affine' + str(i)] = Affine(self.params['W' + str(i)], self.params['b' + str(i)])\n",
    "            self.layers['BatchNormalization' + str(i)] = BatchNormalization()\n",
    "            self.layers['Relu' + str(i)] = Relu()\n",
    "    \n",
    "        self.layers['Affine' + str(self.maxIndex)] = Affine(self.params['W' + str(self.maxIndex)], self.params['b' + str(self.maxIndex)])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x, train_flg=True):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x, train_flg)\n",
    "        return x\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x, False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "\n",
    "        for i in range(1, self.maxIndex+1):\n",
    "            grads['W' + str(i)] = numerical_gradient(loss_W, self.params['W' + str(i)])\n",
    "            grads['b' + str(i)] = numerical_gradient(loss_W, self.params['b' + str(i)])\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        # 계층 레이어의 값을 리스트로 변환하여 가져옴 -> 해당 리스트를 거꾸로 정렬 -> 순서대로 계층의 역전파를 실행\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 기울기\n",
    "        grads = {}\n",
    "        for i in range(1, self.maxIndex+1):\n",
    "            grads['W' + str(i)], grads['b' + str(i)] = self.layers['Affine' + str(i)].dW, self.layers['Affine' + str(i)].db\n",
    "        \n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0670994e-f3c1-40cd-aa98-54d9c43b649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f39baea5-ee85-48f6-8e62-8354bf3a044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4f28e4a-7fd0-4aa5-b46b-fc7fa15f99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03aa10f9-6ad6-45cb-90de-30526a25eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e4e3068-2456-4301-9f51-956e032f3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bac8b5d5-172c-4d85-b11a-47e141c19710",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4eb85f2-7534-48d2-a079-75bf36710c6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 35\u001b[0m\n\u001b[0;32m     27\u001b[0m network \u001b[38;5;241m=\u001b[39m SimpleConvNet(input_dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m), \n\u001b[0;32m     28\u001b[0m                         conv_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m     29\u001b[0m                         hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, weight_init_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[0;32m     32\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mmax_epochs, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     33\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[0;32m     34\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 매개변수 보존\u001b[39;00m\n\u001b[0;32m     38\u001b[0m network\u001b[38;5;241m.\u001b[39msave_params(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 65\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m---> 65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     67\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "Cell \u001b[1;32mIn[40], line 38\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train[batch_mask]\n\u001b[0;32m     36\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[batch_mask]\n\u001b[1;32m---> 38\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "Cell \u001b[1;32mIn[36], line 120\u001b[0m, in \u001b[0;36mSimpleConvNet.gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"기울기를 구한다(오차역전파법).\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    grads['b1']、grads['b2']、... 각 층의 편향\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m    123\u001b[0m dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[36], line 65\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"손실 함수를 구한다.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    t : 정답 레이블\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "Cell \u001b[1;32mIn[36], line 53\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 53\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\202284026_인공지능\\13주차\\common\\layers.py:257\u001b[0m, in \u001b[0;36mPooling.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 257\u001b[0m     N, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    258\u001b[0m     out_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (H \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_h) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[0;32m    259\u001b[0m     out_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (W \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_w) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Ensure the input data has the correct shape\n",
    "x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "x_test = x_test.reshape(-1, 1, 28, 28)\n",
    "\n",
    "iters_num = 10000\n",
    "batch_size = 500\n",
    "#hidden_size = 15 \n",
    "hidden_size = [15, 15, 15, 15, 15]\n",
    "learning_rate = 0.015\n",
    "\n",
    "# 기울기 최적화 함수들\n",
    "sgd = SGD()\n",
    "momentum = Momentum()\n",
    "adaGrad = AdaGrad()\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "#network = TwoLayerNet(input_size=784, hidden_size=hidden_size, output_size=10)\n",
    "max_epochs = 20\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식\n",
    "    \n",
    "    # 일반 갱신\n",
    "    #for key in ('W1', 'b1', 'W2', 'b2'): network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # SGD 학습, 모멘텀 학습, AdaGrad 학습\n",
    "    #sgd.update(network.params, grad)\n",
    "    momentum.update(network.params, grad)\n",
    "    #adaGrad.update(network.params, grad)\n",
    "    \n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"%2d 번째 학습(에폭) --> train_acc : %.2f%%, test_acc : %.2f%%\" % (len(train_acc_list), train_acc*100, test_acc*100))\n",
    "        \n",
    "print(\"최종 학습률 --> train_acc : %.2f%%, test_acc : %.2f%%\" % (train_acc_list[-1]*100, test_acc_list[-1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70fef2-d5d5-4457-93bc-9025cb52b372",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc8134-41c7-41dc-b827-1d09d6e5d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def smooth_curve(x):\n",
    "    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용\n",
    "    \n",
    "    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
    "    \"\"\"\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45435b-1346-4d4d-9209-333d7388fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 값\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, smooth_curve(train_loss_list), label='Momentum')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "63b98c3d-ff45-44f4-ae16-66d859c1ae85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 에폭 당 정확도\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff75f82-f5d8-4a70-81d5-e2ec14c98898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
